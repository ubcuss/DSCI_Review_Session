{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c017d9-e796-4255-a2ff-df5491f323fb",
   "metadata": {},
   "source": [
    "# DSCI 100 Review Session 2 Worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ddca0-4e95-4025-8dce-27349ec92db8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading relevant packages for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3df9a3-29d1-442d-9d31-22908ce0be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tidyverse' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'tidyr' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 4.3.3\"\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.1     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Warning message:\n",
      "\"package 'repr' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'tidymodels' was built under R version 4.3.3\"\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────────────────────────────────────────────── tidymodels 1.2.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.5      \u001b[32m✔\u001b[39m \u001b[34mrsample     \u001b[39m 1.2.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials       \u001b[39m 1.2.1      \u001b[32m✔\u001b[39m \u001b[34mtune        \u001b[39m 1.2.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer       \u001b[39m 1.0.7      \u001b[32m✔\u001b[39m \u001b[34mworkflows   \u001b[39m 1.1.4 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.3.0      \u001b[32m✔\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.1.0 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip     \u001b[39m 1.2.1      \u001b[32m✔\u001b[39m \u001b[34myardstick   \u001b[39m 1.3.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.10     \n",
      "\n",
      "Warning message:\n",
      "\"package 'dials' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'scales' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'infer' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'modeldata' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'parsnip' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'recipes' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'rsample' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'tune' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'workflows' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'workflowsets' was built under R version 4.3.3\"\n",
      "Warning message:\n",
      "\"package 'yardstick' was built under R version 4.3.3\"\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m•\u001b[39m Use suppressPackageStartupMessages() to eliminate package startup messages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Otherwise I highly recommend using DSCI100 jupyterlab (access it through canvas) since packages are already up to date.\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "options(repr.matrix.max.rows = 6) #limits output of dataframes to 6 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75bb9d-4d20-4c78-81c5-26aab73b5899",
   "metadata": {},
   "source": [
    "## Chapter 6: Classification I (Training and Predicting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20261f9-6f4d-4a41-9af4-48f96e692307",
   "metadata": {},
   "source": [
    "### 6.0 Important packages for chapter 6\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a473a02-666c-4ffd-806d-38a4726d0162",
   "metadata": {},
   "source": [
    "* `tidymodels`\n",
    "    * K-nearest neighbour algorithm is implemented in the parsnip PACKAGE included in the tidymodels package collection.\n",
    "    * The tidymodels package collection also provides the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c6cd5-1a10-4f4c-ad1d-7cb45550cb78",
   "metadata": {},
   "source": [
    "### 6.1 Classification and Training Sets\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff1ff8-91c4-4ef6-b95a-1d45f386b534",
   "metadata": {},
   "source": [
    "* Classification is predicting a **categorical class** (sometimes called a label) for an observation given its other **quantitative variables** (sometimes called features). \n",
    "* Generally, a classifier assigns an observation (e.g. a new patient) to a class (e.g. diseased or healthy) on the basis of how similar it is to other observations for which we know the class (e.g. previous patients with known diseases and symptoms).\n",
    "* These observations with known classes that we use as a basis for prediction are called a training set.\n",
    "* We call them a “training set” because we use these observations to train, or teach, our classifier so that we can use it to make predictions on new data that we have not seen previously.\n",
    "\n",
    "> **Training Data/Set:** It is a collection of observations for which we know the true classes. It can be used to explore and build our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6bdf4-060c-46a9-87cd-3592161bf296",
   "metadata": {},
   "source": [
    "### 6.3 K-Nearest Neighbours Algorithm\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a78374-2ba2-463f-8ffa-098cd4963f16",
   "metadata": {},
   "source": [
    "In order to classify a new observation using a K-nearest neighbour classifier, we have to:\n",
    "\n",
    "1) Compute the distance between the new observation and each observation in the training set  \n",
    "2) Sort the data table in ascending order according to the distances  \n",
    "3) Choose the top K rows of the sorted table  \n",
    "4) Classify the new observation based on a majority vote of the neighbour classes  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07f035-4031-4ebf-bec9-07af58e5d397",
   "metadata": {},
   "source": [
    "#### **<u>Example Code:</u>**\n",
    "\n",
    "```\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 5) |>  #(1)\n",
    "        \tset_engine(\"kknn\") |>                                            #(2)\n",
    "        \tset_mode(\"classification\")                                       #(3)\n",
    "\n",
    "knn_fit <- fit(knn_spec, target_variable ~ predictor_variables, df)          #(4)\n",
    "\n",
    "new_obs <- tibble(Permimeter = 0, Concavity = 3.5)                           #(5)\n",
    "predict(knn_fit, new_obs)                                                    #(6)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056f171-7ae5-4156-b331-98fe8964f30d",
   "metadata": {},
   "source": [
    "1) We create a model specification for K-nearest neighbours classification by calling the `nearest_neighbor()` function.  \n",
    "   Here we specify that we want to use K=5 neighbours.  \n",
    "   The `weight_func` argument controls how neighbours vote when classifying a new observation  \n",
    "   By setting it to `\"rectangular\"`, it measures the straight-line distance.\n",
    "   Each of the K nearest neighbours gets exactly 1 vote as described above.\n",
    "\n",
    "2) The `set_engine()` trains the model with a particular computational engine which needs to be specified in its argument.  \n",
    "   In this case, we specify the `\"kknn\"` engine\n",
    "\n",
    "3) The `setmode()` specifies what type of problem this is in its argument  \n",
    "   In this case, it's a `\"classification\"` problem.  \n",
    "\n",
    "4) In order to fit the model on the breast cancer data, we need to pass the model specification as the 1st argument.  \n",
    "   Specify the variables we need to use to make the prediction and what variable to use as the target in the 2nd argument.  \n",
    "   (Note: if you want to use all the other variables/columns to predict, then type `target_variable ~ .` )  \n",
    "   The data frame being used as the 3rd argument.  \n",
    "   The fit object lists the function that trains the model as well as the “best” settings for the number of neighbours and weight function  \n",
    "\n",
    "5) `new_obs <- tibble(x_col = ..., y_col = ...)` creates a new observation with the x and y values.\n",
    "\n",
    "6) prediction is made on the new observation using the fitted model and the new observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4696c1-5d51-4d6f-9848-24777d84774b",
   "metadata": {},
   "source": [
    "#### 6.3.1 Common problems using K-NN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf96f4f-835b-4b2d-b83c-b38cfae79ab6",
   "metadata": {},
   "source": [
    "1) **Varying scales of each variable**  \n",
    "When using K-nearest neighbour classification, the scale of each variable matters since large scale variables can have a greater (unwanted) affects.\n",
    "\n",
    "2) **Class Imbalance**  \n",
    "Another potential issue in a data set for a classifier is class imbalance, i.e., when one label is much more common than another.  \n",
    "If there are many more data points with one label overall, the algorithm is more likely to pick that label in general \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d891f-89f4-4180-b335-4ad8b24bbc1a",
   "metadata": {},
   "source": [
    "#### 6.3.2 Solution to these problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e78d6-7ed4-4dd4-8521-a3bd49b42cb7",
   "metadata": {},
   "source": [
    "1) **Scaling and Centering**  \n",
    "When all variables in a data set have a mean (center) of 0 and a standard deviation (scale) of 1, we say that the data have been standardized.  \n",
    "As a rule of thumb, standardizing your data should be a part of the preprocessing you do before any predictive modelling / analysis.\n",
    "\n",
    "2) **Balancing**  \n",
    "Rebalance the data by oversampling the rare class.  \n",
    "We replicate rare observations multiple times in our data set to give them more voting power in the K-nearest neighbour algorithm.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadbbe9-31a5-4189-ad2d-5c8c15425373",
   "metadata": {},
   "source": [
    "#### 6.3.3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18ec27-7b9f-427b-b9e3-c1e8476527fb",
   "metadata": {},
   "source": [
    "> **Note:** \n",
    ">* Scaling & Centering and Balancing are part of preprocessing the data\n",
    ">* In the `tidymodels` framework, data preprocessing is done by using a Recipe.\n",
    ">* `prep()` and `bake()` are used in conjunction with `recipe()` to preprocess data (e.g. centering and scaling data). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0617477-a2e6-4ac1-9d97-9ace030e59ce",
   "metadata": {},
   "source": [
    "**Explanation of the `prep()`, `bake()` workflow:**\n",
    "- `prep()` calculates the standard deviations and means required to scale and center the data. If you run the recipe before `prep()`, it just mentions the pre-processing steps it has to take.\n",
    "- `bake()` applies the results of `prep()` on to the data. \n",
    "- You might be wondering, \"why are these two separate functions, then?\". Well, you might want to calculate the standard deviations and means for one data set and use those numbers to scale a DIFFERENT data set.\n",
    "- For example, you might want to find the standard deviations for the training data set and use that to scale the testing data set. \n",
    "- This is because training our model or even standardizing our data based on the test data jeopardizes the validity of the test data and violates the golden rule of machine learning: never use any part of the test data to help make your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457a59b-f4e3-464f-9dfb-cfa40accceb4",
   "metadata": {},
   "source": [
    "#### 6.3.4 Scaling and Centering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a064cf4-b5b7-4281-8873-0e8424120573",
   "metadata": {},
   "source": [
    "* When all variables in a data set have a mean (center) of 0 and a standard deviation (scale) of 1, we say that the data have been standardized.\n",
    "* As a rule of thumb, standardizing your data should be a part of the preprocessing you do before any predictive modelling / analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274bf36b-c3b6-4a69-bfdf-06c0dc33e524",
   "metadata": {},
   "source": [
    "##### **<u>Example Code:</u>**\n",
    "\n",
    "```\n",
    "udf_recipe <- recipe(target_col ~ ., df) |>    #(1)\n",
    "  step_scale(all_predictors()) |>              #(2)\n",
    "  step_center(all_predictors()) |>             #(3)\n",
    "  prep()                                       #(4)\n",
    "\n",
    "scaled_df <- bake(recipe, df)                  #(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d720a-6397-400b-a158-a9fd0b890a97",
   "metadata": {},
   "source": [
    "1) `recipe()` creates a Recipe for Preprocessing Data. Here we specify the target column/variable, and all other variables are predictors. (udf stands for unscaled dataframe)\n",
    "\n",
    "2) `step_scale()` scales numeric data. `all_predictors()` applies it to all the predictor variables/columns.\n",
    "\n",
    "3) `step_center()` centers numeric data.\n",
    "\n",
    "4) `prep()` function finalizes the recipe by using the data to compute anything necessary to run the recipe (in this case, the column means and standard deviations).\n",
    "\n",
    "5) `bake()` function applies the recipe to the dataframe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a0d88-05c1-46cc-932d-5c5224a1df13",
   "metadata": {},
   "source": [
    "#### 6.3.5 Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fc496-a5f4-4083-a103-e57630449076",
   "metadata": {},
   "source": [
    "* Rebalance the data by oversampling the rare class.\n",
    "* We will replicate rare observations multiple times in our data set to give them more voting power in the K-nearest neighbour algorithm.\n",
    "* In order to do this, we will add an oversampling step to the earlier `udf_recipe` recipe with the `step_upsample()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f30e83-05d9-4eb1-aebb-19d292ad63e8",
   "metadata": {},
   "source": [
    "##### **<u>Example Code:</u>**\n",
    "\n",
    "```\n",
    "ups_recipe <- recipe(target_col ~ ., data = df) |>          #(1)\n",
    "  step_upsample(target_col, over_ratio = n, df) |>          #(2)\n",
    "  prep()                                                    #(3)\n",
    "\n",
    "upsampled_df <- bake(ups_recipe, df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364ce76-6172-426a-9f44-5ada46454bef",
   "metadata": {},
   "source": [
    "1) `recipe()` creates a Recipe for Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62ba89-571e-420d-aa04-566af81a27fc",
   "metadata": {},
   "source": [
    "2) `step_upsample()`\n",
    "    * oversamples data points in minority to match those of majority.\n",
    "    * 1st argument selects the `target_col`.\n",
    "    * 2nd argument is a numeric value for the ratio of the majority-to-minority frequencies. (DEFAULT: over_ratio = 1)\n",
    "    * 3rd argument takes in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1f4fa-9cb3-4bbf-8b7d-8288338d6062",
   "metadata": {},
   "source": [
    "3) `prep()` function finalizes the recipe by using the data to compute anything necessary to run the recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8984846-0591-49bf-830b-4c162c2c3dc1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1423b03-f6e4-4466-9ca1-e2d3e46bdc6d",
   "metadata": {},
   "source": [
    "### 6.4 `workflow()`\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f8c73-93c5-478b-8d2e-2b0f71116288",
   "metadata": {},
   "source": [
    ">* We’re going to use this recipe in a `workflow()` so we don’t need to stress a lot about whether to `prep()` or not. \n",
    ">* If you want to explore what the recipe is doing to your data:\n",
    "    * You can first `prep()` the recipe to estimate the parameters needed for each step\n",
    "    * Then `bake(new_data = NULL)` to pull out the training data with those steps applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f9444-6977-46d8-b913-53f6ea897a21",
   "metadata": {},
   "source": [
    "##### **<u>Example Code:</u>**\n",
    "```\n",
    "# load the unscaled cancer data and make sure the target Class variable is a factor\n",
    "unscaled_cancer <- read_csv(\"data/unscaled_wdbc.csv\") |> \n",
    "  mutate(Class = as_factor(Class))\n",
    "\n",
    "# create the KNN model\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 7) |> \n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# create the centering / scaling recipe\n",
    "uc_recipe <- recipe(Class ~ Area + Smoothness, data = unscaled_cancer) |> \n",
    "  step_scale(all_predictors()) |> \n",
    "  step_center(all_predictors())\n",
    "\n",
    "knn_fit <- workflow() |> \n",
    "  add_recipe(uc_recipe) |> \n",
    "  add_model(knn_spec) |> \n",
    "  fit(data = unscaled_cancer)\n",
    "\n",
    "knn_fit\n",
    "\n",
    "new_obs <- tibble(Perimeter = 0, Concavity = 3.5)\n",
    "knnPred <- predict(knn_fit, new_obsv)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26f8fe-99c8-47f0-9259-5b17d78bdf8a",
   "metadata": {},
   "source": [
    "#### 6.4.1 Advantage of using `workflow()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61acc5-f7aa-4b33-9c92-b2db5fea1ee8",
   "metadata": {},
   "source": [
    "- This is a simple way to chain together multiple data analysis steps without a lot of otherwise necessary code for intermediate steps.\n",
    "- We did not use the select function to extract the relevant variables from the data frame, and instead simply specified the relevant variables to use via the formula `Class ~ Area + Smoothness` (instead of `Class ~ .`) in the recipe.\n",
    "- You will also notice that we did not call `prep()` on the recipe; this is unnecssary when it is placed in a workflow.\n",
    "- We do not include a formula in the fit function. This is again because we included the formula in the recipe, so there is no need to respecify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a57be-20b1-430c-9f3d-89179628ac9d",
   "metadata": {},
   "source": [
    "## Chapter 7: Classification II (Evaluation and Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cd414-ac3c-423c-a497-d902bdcad396",
   "metadata": {},
   "source": [
    "### 7.1 Common functions we may use in this chapter\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79824f-6156-48df-9547-c0a0dceec0ca",
   "metadata": {},
   "source": [
    "* `bind_cols(col_object, df)`\n",
    "    * binds the column/vector in argument 1 to dataframe in argument 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02178ac-4e7c-4bc3-91e8-156b32ed72d2",
   "metadata": {},
   "source": [
    "* `rename(df, new_col_name = old_col_name)`\n",
    "    * renames column name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8b52e-63e8-4f72-a0bc-0929b77156c8",
   "metadata": {},
   "source": [
    "### 7.2 Measures to assess the classifier\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c3b1d-1e25-4925-b230-0c35589b94dc",
   "metadata": {},
   "source": [
    "1) **Prediction Accuracy**:\n",
    "$\\frac{\\text{total number of correct predictions}}{\\text{total number of predictions}}$  \n",
    "2) **Precision**  \n",
    "3) **Recall**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfec3de-157c-4216-a81f-c718c24ceab8",
   "metadata": {},
   "source": [
    "### 7.3 Steps to assess the classifier\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac63a75-97be-43df-a60b-3f083afb056b",
   "metadata": {},
   "source": [
    "##### **(1) <u>Create the TRAIN SET and TEST SET</u>**\n",
    "- Training Set should be a 50-100% split of the data  \n",
    "- Test Set should be the remaining 0-50% of data  \n",
    "- You want to trade off between:\n",
    "    - training an accurate model (by using a larger **training** data set)\n",
    "    - getting an accurate evaluation of its performance (by using a larger **test** data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597cf9d-d96d-433b-84d1-d6c5db71a5f0",
   "metadata": {},
   "source": [
    "- `initial_split(df, prop = ..., strata = target_column)`\n",
    "    - 2nd argument is the proportion you want for training (e.g. 0.75)\n",
    "    - 3rd argument is the column name of the target variable.\n",
    "    - use `set.seed()` for reproducible results as `initial_split()` randomly samples from df.\n",
    "    - use `training(split_object)` & `testing(split_object)` to assign the training and test sets to respective reference objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180f5c7-ff80-4f8e-8959-e93a84702a3c",
   "metadata": {},
   "source": [
    "##### **(2) <u>Pre-Process the data</u>**\n",
    "- As we mentioned last chapter, K-NN is sensitive to the scale of the predictors, and so we should perform some preprocessing to standardize them.\n",
    "- We should create the standardization preprocessor **using only the training data**.  \n",
    "(This ensures that our test data does not influence any aspect of our model training)\n",
    "- Once we have created the standardization preprocessor, we can then apply it **separately** to both the **training** and **test** datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17bf50c-bd15-459a-8c9f-7cb145233ac9",
   "metadata": {},
   "source": [
    "##### **(3) <u>Train the Classifier</u>**\n",
    "- Create the K-nearest neighbour classifier with **only** the **training set**.  \n",
    "(Here again you see the set.seed function. In the K-nearest neighbour algorithm, if there is a tie for the majority neighbour class, the winner is randomly selected.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d2c36-5e14-4faa-ae0c-7745315b407e",
   "metadata": {},
   "source": [
    "##### **(4) <u>Create the labels in the Test set</u>**\n",
    "- Predict the class labels for our **test set** using the `predict()` function\n",
    "- use the `bind_cols()` to add the column of predictions to the original test data creating the predictions dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7d136-9807-4036-84d5-a5bbb17db1e6",
   "metadata": {},
   "source": [
    "##### **(5) <u>Compute the accuracy</u>**\n",
    "- To assess classifier's accuracy, we use the `metrics()` function.\n",
    "- `metrics(df, truth = target_col_name, estimate = .pred_class)`\n",
    "    - 2nd argument takes the name of the the target variable/column\n",
    "    - 3rd argument takes the name of the column with the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e14e2-e812-4196-bc12-8b460264071a",
   "metadata": {},
   "source": [
    "- We can also look at the confusion matrix for the classifier, which shows the table of predicted labels and correct labels, using the `conf_mat()`.\n",
    "- The *confusion matrix* for the classifier will show us the table of predicted labels and correct labels. \n",
    "\n",
    "A confusion matrix is essentially a classification matrix. The columns of the confusion matrix represent the actual class and the rows represent the predicted class (or vice versa). Shown below is an example of a confusion matrix.\n",
    "\n",
    "|                  |          |  Actual Values |                |\n",
    "|:----------------:|----------|:--------------:|:--------------:|\n",
    "|                  |          |    Positive    |    Negative    |\n",
    "|**Predicted Value**  | Positive |  True Positive | False Positive|\n",
    "|                  | Negative | False Negative | True Negative  |\n",
    "\n",
    "\n",
    "- A **true positive** is an outcome where the model correctly predicts the positive class.\n",
    "- A **true negative** is an outcome where the model correctly predicts the negative class.\n",
    "- A **false positive** is an outcome where the model incorrectly predicts the positive class.\n",
    "- A **false negative** is an outcome where the model incorrectly predicts the negative class.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can create a confusion matrix by using the `conf_mat` function. Similar to the `metrics` function, you will have to specify the `truth` and `estimate` arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36fd40-a5e5-4c46-b626-d358a83e31d5",
   "metadata": {},
   "source": [
    "- `conf_mat(df, truth = Class, estimate = .pred_class)`\n",
    "    - 2nd argument takes the name of the the target variable/column\n",
    "    - 3rd argument takes the name of the column with the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e0409-2c8a-48b4-8a18-24c0c109f7a9",
   "metadata": {},
   "source": [
    "### 7.4 Tuning the model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147be89c-0cf8-40c7-b578-5544fa3fd52a",
   "metadata": {},
   "source": [
    "- Predictive models in statistics and machine learning have parameters that you have to pick.\n",
    "- For example, in the K-nearest neighbour classification algorithm we have had to pick the number of neighbours K for the class vote.\n",
    "- Making the most optimal selection is called **Tuning** the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e61764-65a8-4e43-96ef-380177347ef4",
   "metadata": {},
   "source": [
    "#### 7.4.1 Cross Validation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48f467-8ae1-435c-9998-efdced08f409",
   "metadata": {},
   "source": [
    "- Instead of randomly splitting the data, we want each observation in the data set to be used in a validation set only a single time.\n",
    "- The name for this strategy is called cross-validation.\n",
    "- In cross-validation, we split our overall **Training data** into $V$ evenly-sized chunks/folds\n",
    "- Then iteratively use 1 chunk as the **Validation set** and combine the remaining $V−1$ chunks as the **Training (sub)set**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fd1f1-9046-43b0-ad64-aca822aff712",
   "metadata": {},
   "source": [
    "$$\\text{Cross-validation accuracy} = \\frac{\\sum{\\text{accuracy of n folds}}}{\\text{number of folds}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcb78b-d57e-40b5-876e-392e57ea7437",
   "metadata": {},
   "source": [
    "### 7.5 Underfitting and Overfitting\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147ec5d-1189-4474-8071-8f7a2a349df1",
   "metadata": {},
   "source": [
    "<u>**Under-fitting:**</u>  \n",
    "As we increase the number of neighbours, more and more of the training observations (and those that are farther and farther away from the point)\n",
    "get a “say” in what the class of a new observation is. This causes a sort of “averaging effect” to take place, making the boundary between where\n",
    "our classifier would predict a tumour to be malignant versus benign to smooth out and become simpler.\n",
    "\n",
    "In general, if the model isn’t influenced enough by the training data, it is said to underfit the data.\n",
    "\n",
    "\n",
    "<u>**Over-fitting:**</u>  \n",
    "In contrast, when we decrease the number of neighbours, each individual data point has a stronger and stronger vote regarding nearby points. \n",
    "Since the data themselves are noisy, this causes a more “jagged” boundary corresponding to a less simple model.\n",
    "This is just as problematic as the large $K$ case, because the classifier becomes unreliable on new data:\n",
    "if we had a different training set, the predictions would be completely different.\n",
    "\n",
    "In general, if the model is influenced too much by the training data, it is said to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6385f-a45c-4f28-851c-bb5cb012f621",
   "metadata": {},
   "source": [
    "<center><img src=\"data_2/underover.JPG\" width=500 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68520f-79d8-4a3f-955a-1855eccf9b72",
   "metadata": {},
   "source": [
    "### 7.7 Advantages and Disadvantages of K-NN Classification\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50728ee-8581-4149-ad6e-21b83f329995",
   "metadata": {},
   "source": [
    "<u>**Advantages:**</u>  \n",
    "* Simple and easy to understand\n",
    "* No assumptions about what the data must look like\n",
    "* Works easily for binary (two-class) and multi-class (> 2 classes) classification problems\n",
    "\n",
    "\n",
    "<u>**Disadvantages:**</u>  \n",
    "* As data gets bigger and bigger, K-nearest neighbour gets slower and slower, quite quickly\n",
    "* Does not perform well with a large number of predictors\n",
    "* Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf9019-4735-4dfe-bd91-3232d45a2af5",
   "metadata": {},
   "source": [
    "## Chapter 8: Regression I (K-NN Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044e756-d58d-4fda-9c5e-f0b04d791560",
   "metadata": {},
   "source": [
    "### 8.1 Introduction to K-NN regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d6de9-332b-41ac-a9c1-09ab0986cf25",
   "metadata": {},
   "source": [
    "* Regression, like classification, is a predictive problem setting where we want to use past information to predict future observations.\n",
    "* The goal is to predict numerical values instead of class labels.\n",
    "* To predict a value of $Y$ for a new observation using k-nn regression, we identify the k-nearest neighbours and then assign it the mean of the k-nearest neighbours as the predicted value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f5e0b-2586-4397-9c79-32a2b504932c",
   "metadata": {},
   "source": [
    "#### 8.1.1 Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6932234-e073-411d-b100-d9f2e287197b",
   "metadata": {},
   "source": [
    "* Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fa032-e26c-41b4-8454-b15d5226b655",
   "metadata": {},
   "source": [
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{\\left(\\hat{y_i}-{y_i}\\right)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b33787-ee40-4ca9-90de-5b64d427b461",
   "metadata": {},
   "source": [
    "#### 8.1.2 RMSE vs RMSPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2674bf1-1747-4c2b-8fbe-b8108514ba2b",
   "metadata": {},
   "source": [
    "* When predicting and evaluating prediction quality on the **training data**, we say $\\text{RMSE}$.\n",
    "* By contrast, when predicting and evaluating prediction quality on the **testing data** or **validation data**, we say $\\text{RMSPE}$.\n",
    "> $\\text{RMSE}$ is a measure of goodness of fit.  \n",
    "$\\text{RMSE}$ measures how well the model predicts on data it was trained with.  \n",
    "$\\text{RMSPE}$ is a measure of prediction quality.  \n",
    "$\\text{RMSPE}$ measures how well the model predicts on data it was not trained with.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa3a7d-0de3-4faf-8cdf-96be817c9a54",
   "metadata": {},
   "source": [
    "### 8.3 Strength and Limitations of K-NN Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe14d4-763b-47c2-af6c-ccab35f275f0",
   "metadata": {},
   "source": [
    "<u>**Strengths:**</u>  \n",
    "1) Simple and easy to understand  \n",
    "2) No assumptions about what the data must look like  \n",
    "3) Works well with non-linear relationships (i.e., if the relationship is not a straight line)  \n",
    "\n",
    "\n",
    "<u>**Limitations:**</u>  \n",
    "1) As data gets bigger and bigger, K-NN gets slower and slower, quite quickly 2. Does not perform well with a large number of predictors unless the size of the training set is exponentially larger.  \n",
    "2) Does not predict well beyond the range of values input in your training data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab388f-7f89-4fb6-b42a-fb4059436533",
   "metadata": {},
   "source": [
    "### 8.4 Overfitting vs Underfitting\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba2821-f43b-4639-9126-b5cbe673ef34",
   "metadata": {},
   "source": [
    "<u>**Overfitting:**</u>  \n",
    "Creates high variance and low bias.  \n",
    "It has high variance because the flexible blue line follows the training observations very closely, \n",
    "and if we were to change any one of the training observation data points we would change the flexible blue line quite a lot. \n",
    "This means that the blue line matches the data we happen to have in this training data set, however, \n",
    "if we were to collect another training data set from the Sacramento real estate market it likely wouldn’t match those observations as well.\n",
    "\n",
    "\n",
    "<u>**Underfitting:**</u>  \n",
    "Creates low variance and high bias as the blue line is extremely smooth, and almost flat.  \n",
    "This happens because our predicted values for a given x value (here home size), depend on many many (450) neighbouring observations.  \n",
    "A model like this has low variance and high bias (intuitively, it provides very reliable, but generally very inaccurate predictions).  \n",
    "It has low variance because the smooth, inflexible blue line does not follow the training observations very closely, and if we were to change any one of the training observation data points it really wouldn’t affect the shape of the smooth blue line at all.  \n",
    "This means that although the blue line matches does not match the data we happen to have in this particular training data set perfectly, if we were to collect another training data set from the Sacramento real estate market it likely would match those observations equally as well as it matches those in this training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67a655-1487-4f7a-9724-d4676dc5d656",
   "metadata": {},
   "source": [
    "<center><img src=\"data_2/underover.JPG\" width=500 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f0d4b-ef12-439e-a141-ae58f9e21401",
   "metadata": {},
   "source": [
    "## Chapter 9: Regression II (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b619d-139b-46d5-82d6-4ed2c95fc5e5",
   "metadata": {},
   "source": [
    "### 9.0 Important Packages for Chapter 9\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45befcb-3e5b-4a5d-9d91-7bb360803164",
   "metadata": {},
   "source": [
    "* `tidymodels`\n",
    "    * We can perform simple linear regression in R using tidymodels in a very similar manner to how we performed KNN regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb742780-775a-401e-b1b8-052e4bde7b18",
   "metadata": {},
   "source": [
    "### 9.1 Introduction to Linear Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e87c0-f327-4227-9fbc-627820655ced",
   "metadata": {},
   "source": [
    "- In KNN regression, we look at the K nearest neighbours and average over their values for a prediction. \n",
    "- In simple linear regression, we create a straight line of best fit through the training data and then “look up” the prediction using the line.\n",
    "  Therefore using the data to find the line of best fit is equivalent to finding coefficients $\\beta_{0}$ and $\\beta_{1}$ that parametrize (correspond to) the line of best fit. \n",
    "- Simple linear regression chooses the straight line of best fit by choosing the line that minimizes the average squared vertical distance between itself and each of the observed data points in the training data.\n",
    "- To assess the predictive accuracy of a simple linear regression model, we use $\\text{RMSPE}$, the same measure of predictive performance we used with KNN regression.\n",
    "- An additional difference that you will notice below is that we do not standardize (i.e., scale and center) our predictors.\n",
    "\n",
    "<p><center>\n",
    "  <img src = \"https://miro.medium.com/max/611/1*jopCO2kMEI84s6fiGKdXqg.png\" width = \"500\"/>\n",
    "</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd07d26-4080-4623-b9fd-23229d6bceb8",
   "metadata": {},
   "source": [
    "### 9.3 Comparison of Linear Regression vs KNN Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41269f5f-6352-466e-ae7a-83f30456139d",
   "metadata": {},
   "source": [
    "<u>**Advantages of Linear Regression over KNN-regression:**</u>  \n",
    "1) KNN regression does **NOT** predict well beyond the range of the predictors in the training data. Linear regression can be used to address this problem.\n",
    "2) In KNN regression, the method gets significantly slower as the training dataset grows. Linear regression can be used to address this problem.\n",
    "3) In linear regression, standardization does not affect the fit (it does affect the coefficients in the equation, though!)\n",
    "4) A straight line can be defined by two numbers, the vertical intercept and the slope. The intercept tells us what the prediction is when all of the predictors are equal to 0; and the slope tells us what unit increase in the target/response variable we predict given a unit increase in the predictor variable. KNN regression, as simple as it is to implement and understand, has no such interpretability from its wiggly line.\n",
    "\n",
    "<u>**Disadvantages of Linear Regression when compared to KNN-regression:**</u>  \n",
    "1) When the relationship between the target and the predictor is not linear, but instead some other shape (e.g. curved or oscillating).  In these cases the prediction model from a simple linear regression will underfit (have high bias), meaning that model/predicted values does not match the actual observed values very well. Such a model would probably have a quite high $\\text{RMSE}$ when assessing model goodness of fit on the training data and a quite high $\\text{RMSPE}$ when assessing model prediction quality on a test data set.\n",
    "\n",
    "On such a data set, KNN regression may fare better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb171a-d2e8-4b99-94aa-5505fd9a30f6",
   "metadata": {},
   "source": [
    "### 9.4 Extrapolation\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d8aa1-53e2-49ba-93e7-6cb4eb710839",
   "metadata": {},
   "source": [
    "<u>**Extrapolation problems**</u>  \n",
    "* Predicting outside the range of the observed data is known as extrapolation; KNN and linear models behave quite differently when extrapolating.  \n",
    "* Depending on the application, the flat or constant slope trend may make more sense.  \n",
    "* For example, if our housing data were slightly different, the linear model may have actually predicted a negative price for a small houses (if the intercept $\\beta_{0}$ was negative), which obviously does not match reality.  \n",
    "* On the other hand, the trend of increasing house size corresponding to increasing house price probably continues for large houses, so the “flat” extrapolation of KNN likely does not match reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e6552-2987-4554-ab05-1a03fad11cc7",
   "metadata": {},
   "source": [
    "### 9.6 Problems of Linear Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131fb5a-20d4-4a11-814a-94e582e98bfa",
   "metadata": {},
   "source": [
    "1) **Outliers**  \n",
    "The problem with outliers is that they can have too much influence on the line of best fit.\n",
    "\n",
    "2) **Collinearity problem in Multivariate linear regression**  \n",
    "If collinearity between predictors are very high, then the plane of best fit will have regression coefficients that are very sensitive to the exact values in the data.  \n",
    "We can design new predictors (by centering them) to tackle this problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
