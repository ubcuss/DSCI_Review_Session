{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91d3e90-2d63-4b92-a8a0-956dd0c6fd15",
   "metadata": {},
   "source": [
    "# DSCI 100 Review Session 3 Worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722afbe-f380-4b75-ad19-7ba7cde52d30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading relevant packages for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646a90cd-0b48-40a9-adf6-93a0d907a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use your DSCI100 jupyterlab servers for this worksheet.\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(broom)\n",
    "options(repr.matrix.max.rows = 6) #limits output of dataframes to 6 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b7f22-24f9-4ea1-820a-cceb215c7a32",
   "metadata": {},
   "source": [
    "## Chapter 10: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96a31d-b326-4195-a22b-ee396bb3de97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.0 Important packages for chapter 10\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b371b3-e2b9-4ed2-8436-6584feb5d7e9",
   "metadata": {},
   "source": [
    "* `broom`\n",
    "    - provides us with the `augment()` function\n",
    "    - provides us with the `glance()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c92ce-fc56-4036-a190-23188a30151d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.1 Why perform clustering?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543790f-a2ec-4107-a7df-1280356150ba",
   "metadata": {},
   "source": [
    "As part of exploratory data analysis, it is often helpful to see if there are meaningful subgroups (or clusters) in the data; this grouping can be used for many purposes, such as:\n",
    "1) _Generating new questions_\n",
    "2) _Improving predictive analyses_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3418d6-1acb-4369-bc7f-48b8aa07719f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.2 What is clustering?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43af115-9a86-49fa-afdf-0e4619132ccc",
   "metadata": {},
   "source": [
    "- Clustering is a data analysis task involving separating a data set into subgroups of related data.  \n",
    "- We might use clustering to separate:  \n",
    "    - a data set of documents into groups that correspond to topics  \n",
    "    - a data set of human genetic information into groups that correspond to ancestral subpopulations  \n",
    "    - a data set of online customers into groups that correspond to purchasing behaviours. Once the data are separated  \n",
    "\n",
    "\n",
    "- The way to rigorously separate the data into groups is to use a clustering algorithm.  \n",
    "- In this chapter, we will focus on the **K-means algorithm**, a widely-used and often very effective clustering method, combined with the **elbow method** for selecting the number of clusters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef02b46-a3bb-4c33-8c05-3d0fae0b4ff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.3 K-means Algorithm\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ee3c0-2d2f-4ed0-826c-8b56b5de05bb",
   "metadata": {},
   "source": [
    "#### 10.3.1 Measuring Cluster Quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266abca-a35a-43db-aef8-a201145f35b1",
   "metadata": {},
   "source": [
    "- The **K-means algorithm** is a procedure that groups data into $K$ clusters.\n",
    "- It starts with an initial clustering of the data, and then iteratively improves it by making adjustments to the assignment of data to clusters until it cannot improve any further.\n",
    "- In **K-means clustering**, we measure the quality of a cluster by its within-cluster sum-of-squared-distances ($\\text{WSSD}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283999cf-a938-496a-bce5-ffe4450cb117",
   "metadata": {},
   "source": [
    "Computing this involves two steps:\n",
    "1. We find the cluster centers by computing the mean of each variable over data points in the cluster. For example, suppose we have a cluster containing four observations, and we are using two variables, $x$ and $y$, to cluster the data. Then we would compute the $\\mu_x$  and $\\mu_y$ , of the cluster center where:  \n",
    "    - $\\mu_x$ = `mean(x)`  \n",
    "    - $\\mu_y$ = `mean(y)`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffebd1-56bc-4f0e-924f-d3fafb6e6f16",
   "metadata": {},
   "source": [
    "2. The second step in computing the $\\text{WSSD}$ is to add up the squared distance between each point in the cluster and the cluster center. We use the straight-line / Euclidean distance formula that we learned about in the classification chapter:  \n",
    "$$S^{2} = \\sum_{i=1}^{N}\\left( (x_i - \\mu_x)^{2} + (y_i - \\mu_y)^2 \\right)$$  \n",
    "(_$\\text{WSSD}$ and $S^2$ are used interchangeably_)  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2b9ca-d278-477b-b35d-0c5cf8fa2913",
   "metadata": {},
   "source": [
    "#### 10.3.2 The Clustering Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9936c4f-e34e-4884-a2dc-7d290b987b78",
   "metadata": {},
   "source": [
    "1) We begin the _K-means_ algorithm by picking $K$, and uniformly randomly assigning data to the $K$ clusters.\n",
    "\n",
    "2) Then K-means consists of two major steps that attempt to minimize the sum of $\\text{WSSD}$s over all the clusters, i.e. $\\text{total WSSD}$:  \n",
    "i) **Center update**: Compute the center of each cluster.  \n",
    "ii) **Label update**: Reassign each data point i.e. ($x_i$,$y_i$), to the cluster with the nearest center i.e. one of the ($\\mu_x$, $\\mu_y$).  \n",
    "\n",
    "_These two steps are repeated until the cluster assignments no longer change._  \n",
    "\n",
    "3) Unlike the classification and regression models we studied in previous chapters, K-means can get “stuck” in a bad solution.\n",
    "    To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization, and pick the clustering that has the lowest final $\\text{total WSSD}$.  \n",
    "\n",
    "4) In order to cluster data using K-means, we also have to pick the number of clusters, $K$.  \n",
    "    - If we set $K<3$, then the clustering merges separate groups of data; this causes a large $\\text{total WSSD}$, since the cluster center (denoted by an “$x$”) is not close to any of the data in the cluster.  \n",
    "    - If we set $K>3$, the clustering subdivides subgroups of data; this does indeed still decrease the $\\text{total WSSD}$, but by only a diminishing amount.  \n",
    "    - If we plot the $\\text{total WSSD}$ versus the number of clusters, we see that the decrease in $\\text{total WSSD}$ levels off (or forms an “**elbow shape**”) when we reach roughly the right number of clusters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdca44d-5271-4f1f-a376-2edb4c6fe24a",
   "metadata": {},
   "source": [
    "<center><img src=\"data_3/kmeans.png\" width=500 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de8f65-fab6-4dbb-8361-dfb39d04ee47",
   "metadata": {},
   "source": [
    "#### 10.3.3 Data Preprocessing for K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e7dc4-a1e1-465f-b063-03eba014e76c",
   "metadata": {},
   "source": [
    "- **K-means** uses the Euclidean distance to compute how similar data points are to each cluster center.  \n",
    "- Similar to K-nearest neighbours classification and regression, **K-means** clustering uses straight-line distance to decide which points are similar to each other.  \n",
    "- Therefore, the scale of each of the variables in the data will influence which cluster data points end up being assigned.  \n",
    "- To address this problem, we typically **standardize** our data before clustering, which ensures that each variable has a mean of 0 and standard deviation of 1 (scaling can however cause loss of interpretability of the variables once we plot them).  \n",
    "- The `scale()` function in R can be used to do this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7821de2-7e54-4791-8cdb-fc65215ea753",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.4 Common functions we may use in this chapter\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed49f8-0a6c-49f9-a5b2-a6533cca9a32",
   "metadata": {},
   "source": [
    "* `map_df(df, function)`\n",
    "    - 1st argument requires the dataframe whose columns will be transformed.\n",
    "    - 2nd argument requires the specified function to be applied to each column of the dataframe.\n",
    "    - this can be used with the `scale()` function to scale each column of the data frame.\n",
    "    - when using `map_df()` we do not include the parenthesis of the function we are using.\n",
    "    - Eg. `map_df(df, scale)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc79403-d6b8-4738-8ee3-8655fa9ec998",
   "metadata": {},
   "source": [
    "## **OR**\n",
    "Note: I noticed that the UBC DSCI 100 textbook is now using a few different functions to execute the K-Means algorithm. All of these functions are alternatives to the ones in the old version which I did. `map_df()` is the older version while `mutate(df, across(everything(), function))` is the newer version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb0ee3-6747-41db-81b9-e40ac200e962",
   "metadata": {},
   "source": [
    "* `mutate(df, across(everything(), function))`\n",
    "    - This combination of functions allows you to apply a transformation to each column.\n",
    "    - It is an alternative to the above `map_df()` function.\n",
    "    - 1st argument requires the dataframe whose columns will be transformed.\n",
    "    - 2nd argument requires the `across()` function which makes it easy to apply the same transformation to multiple columns, allowing you to use `select()` semantics inside in \"data-masking\" functions like `summarise()` and `mutate()`.\n",
    "        - 1st argument requires the `everything()` function which selects all variable. It is very useful in combination with other tidyselect operators i.e. `across()`\n",
    "        - 2nd argument requires the specified function to be applied to each column of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e692d-eb03-46c6-a55a-e87a1f1da5d1",
   "metadata": {},
   "source": [
    "* `rowwise()`\n",
    "    * Row-wise operations require a special type of grouping where each group consists of a single row.\n",
    "    * Like `group_by()`, `rowwise()` doesn’t really do anything itself; it just changes how the other verbs work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1a1f8-315e-46a5-aa12-ad55526f0610",
   "metadata": {},
   "source": [
    "* `kmeans(scaled_df, nstart = ..., centers = ...)`\n",
    "    * 1st argument requires the scaled dataframe\n",
    "    * `nstart` argument in the kmeans function specifies the number of restarts. ***(this is done to counter bad clustering)***\n",
    "    * **Bad clustering** happens because of the random selection of points which can cause the K-means algorithm to get stuck on a bad initialization and the clusters cannot be subdivided further. To counter this, we can use the `nstart` argument in the kmeans function. `nstart` tells R how many times to run the K-means algorithm using different random starts, and returns the best one.\n",
    "    * centers argument specifies the number of clusters.\n",
    "    * Note that since the K-means algorithm uses a random initialization of assignments, we need to set the random seed to make the clustering reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92163554-8832-49f7-be31-7e603204c9ae",
   "metadata": {},
   "source": [
    "* `augment(kmeans_object, original_data)`\n",
    "    * takes in the model and the original data frame, and returns a data frame with the data and the cluster assignments for each point\n",
    "    * this function helps us to plot and identify different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fd233-919a-4914-8bc7-28b51a258ab9",
   "metadata": {},
   "source": [
    "* `glance(kmeans_object)`\n",
    "    * obtains the clustering statistics (including $\\text{WSSD}$) for  the K-means clustering object.\n",
    "    * can be nested in the `map()` function to obtain clustering statistics for each kmeans object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce32682-2f9a-4f91-a8f0-07aabeca7a3f",
   "metadata": {},
   "source": [
    "* `unnest(glanced)`\n",
    "    * unpacks the data frames into simpler column data types.\n",
    "    * this is used when a data frame containing clustering statistics for each k-means object is created and we want to \"unpack\" these statistics.\n",
    "    * the reason why we have to unpack these statistics is because each value of the column/vector is a list of statistics. Therefore, these lists are nested inside each element of the vector. Each list has a common set of statistics therefore `unnest()` produces new columns/vectors for each of these statistics allowing the columns of the dataframe to have one single value rather than nested lists. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ca751-c459-45d0-bbac-121fe7f3ebb9",
   "metadata": {},
   "source": [
    "## Chapter 11: Inference I (Introduction to Statistical Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594d6ec-5f92-4075-b0b9-dcb44e3f3d48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11.0 Important packages for chapter 11\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6ecad-9e64-4868-bb63-67ceaf37140f",
   "metadata": {},
   "source": [
    "* `infer`\n",
    "    * provides us with the `rep_sample_n()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b9e9d-d3e5-4a29-8132-50489ac74e8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11.1 What is inference?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a004e0-0498-4a19-a05a-8bc2cc1647d1",
   "metadata": {},
   "source": [
    "* Data analysis questions regarding how summaries, patterns, trends, or relationships in a data set extend to the wider population are called inferential questions.  \n",
    "* This chapter will introduce two common techniques in statistical inference:  \n",
    "    1) Point Estimation  \n",
    "    2) Interval Estimation  \n",
    "  \n",
    "- Data analysis questions regarding how summaries, patterns, trends, or relationships in a data set extend to the wider population are called inferential questions.  \n",
    "- If our variable of interest is **Categorical**/**Binary**, the population parameter is likely to be **Population Proportion**.  \n",
    "- If our variable of interest is **Quantitative**/**Continuous**, the population parameter is likely to be **Population Mean**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea173fb-29a5-4fa9-b7fb-5861de83ffd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11.2 Definitions\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b044d56-4bad-4516-8608-0866a0dd2aec",
   "metadata": {},
   "source": [
    "| Terms |  Definitions |\n",
    "|----------------|------------|\n",
    "| <p align=\"left\">Mean | <p align=\"left\">The sum of all of the data observations divided by how many observations there are. |\n",
    "| <p align=\"left\">Median | <p align=\"left\">The middle observation of a sorted variable’s data. |\n",
    "| <p align=\"left\">Variance | <p align=\"left\">The mean of the sum of the squared distances of each observation from the mean value of all observations. |\n",
    "| <p align=\"left\">Standard deviation | <p align=\"left\">The square root of the variance. |\n",
    "| <p align=\"left\">Proportion | <p align=\"left\">The number of entities/object with a specific characteristic divided by the total number of entities/objects. |\n",
    "| <p align=\"left\">Observation |  <p align=\"left\">A quantity or quality (or a set of these) from a single member of a population. |\n",
    "| <p align=\"left\">Population | <p align=\"left\">The entire set of entities/objects of interest. |\n",
    "| <p align=\"left\">Population Parameter | <p align=\"left\">A numerical summary value about the population. <p align=\"left\">_(Directly computing population parameters is often time-consuming and costly, and sometimes impossible)_ |\n",
    "| <p align=\"left\">Sample | <p align=\"left\">A subset of entities/objects in the population |\n",
    "| <p align=\"left\">Point Estimate | <p align=\"left\"> A single-value/statistic calculated from sample data that estimates an unknown population parameter of interest. <p align=\"left\">For example, the sample mean $\\bar{x}$ is a point estimate of the population mean $\\mu$. Similarly, the sample proportion $p$ is a point estimate of the population proportion $P$. <p align=\"left\">_(High variation in the sampling distribution of the sample mean causes point estimate to be unreliable.)_|\n",
    "| <p align=\"left\">Statistical Inference | <p align=\"left\">The process of using a sample to make a conclusion about the broader population from which it is taken is referred to as statistical inference. |\n",
    "| <p align=\"left\">Sample Variablity | <p align=\"left\">Estimates vary from sample to sample due to sampling variability.   |\n",
    "| <p align=\"left\">Sampling Distribution | <p align=\"left\">A distribution of point estimates, where each point estimate was calculated from a different random sample from the same population. |\n",
    "| <p align=\"left\">Random sampling | <p align=\"left\">electing a subset of observations from a population where each observation is equally likely to be selected at any point during the selection process. |\n",
    "| <p align=\"left\">Representative sampling | <p align=\"left\">selecting a subset of observations from a population where the sample’s characteristics are a good representation of the population’s characteristics |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0113e55-0b2f-4e6c-8ad1-d8f14b269826",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11.3 Important Concepts\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc73b5-460c-4089-82a4-7494d139826f",
   "metadata": {},
   "source": [
    "- A **population** has **parameters** (i.e. mean, median, sd)\n",
    "- A **sample** has **statistics/point-estimates** (i.e. sample mean, sample median, sample sd)\n",
    "- **Statistic/point estimate** is an estimate of the **parameter**\n",
    "- A collection of **point estimates** form an **estimator.**\n",
    "- An **estimator** is a random variable whose distribution is the sampling distribution for a population parameter therefore a **point estimate** is an observation of this random variable.\n",
    "- From a **sample**, you can obtain the **estimator** (i.e. sampling distribution) by resampling and calculating point estimates for each re-sample.\n",
    "- The **standard deviation** of the **estimator** (i.e. sampling distribution) is also called the **standard error** of the **estimator.**\n",
    "- **Standard error** quantifies the variation/uncertainty associated with the **point estimates** in the **sampling distribution**.\n",
    "- We use a **sampling distribution** to give a range of plausible values for a **population** **parameter**. This range of values is known as a **confidence interval.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a002bf-1751-41d8-9baf-fb02f6e4c459",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 11.4 Effects of choosing a large sample size\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f57ef-3c94-4b7e-a660-dffb4f939f25",
   "metadata": {},
   "source": [
    "1) The mean of the sample mean (across all samples) is equal to the population mean. In other words, the sampling distribution is centred at the population mean.\n",
    "2) Increasing the size of the sample decreases the spread (i.e., the variability) of the sampling distribution making it more narrow. Therefore, a larger sample size results in a more reliable point estimate of the population parameter.\n",
    "3) The distribution of the sample mean is roughly bell-shaped once the sample size is large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49246b4d-a29e-43dd-963a-e8da192e982b",
   "metadata": {},
   "source": [
    "## Chapter 12: Inference II (Bootstrapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc0a04-af5a-4583-b56c-fd4abc9a6857",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 12.1 What is Bootstrapping?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a1d95-5732-4657-bc60-e540d2cf1433",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Bootstrapping is the idea of sampling from our original sample with replacement (also called resampling **with replacement**) to generate a bootstrap sampling distribution.\n",
    "- Sampling with replacement means that each time we choose an observation from the population or sample, we return it before randomly selecting another.\n",
    "- With this procedure, the original sample acts as an estimate of the population, and resampling with replacement gives us enough samples and results in enough sampling variation necessary to produce an approximation of the sampling distributions we have generated in the previous weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10573d-39d4-4499-bdfc-ae8a15e27eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 12.2 Why Bootstrapping?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8498fe34-c251-43d9-ba21-eb80fa5f82a8",
   "metadata": {},
   "source": [
    "- In **Inference I**, we had access to the population which allowed us to evaluate how accurate the estimate was, and even get a sense of how much the estimate would vary for different samples from the population.\n",
    "- Often times we do not have access to the population of a variable.\n",
    "- Remember that the sampling distribution allows us to study sampling variation (i.e. range of plausible values for the estimator).\n",
    "- Since the (true) sampling distribution of an estimator can only be formed by taking repeated samples from the population, if we do not have access to the population, we can not create the sampling distribution.\n",
    "- In this case, bootstrapping (or specifically, **bootstrap resampling with replacement)** can be very useful.\n",
    "- Bootstrapping allows us to study sampling variation without taking more samples from the population to produce a sampling distribution that approximates the true sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733a527-6eef-4b4d-9dfa-ec5e68a6d31e",
   "metadata": {},
   "source": [
    "### 12.3 How to create a Bootstrap (sampling) distribution from a single sample\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ddc41-faea-4d86-bb19-890e9b482197",
   "metadata": {},
   "source": [
    "For a sample of size $n$, you:\n",
    "\n",
    "1. Randomly select an observation from the original sample, which was drawn from the population\n",
    "2. Record the observation’s value\n",
    "3. Replace that observation\n",
    "4. Repeat steps 1 - 3 (sampling with replacement) until you have $n$ observations, which form a bootstrap sample\n",
    "> Steps 1-4 give you one bootstrap sample.  \n",
    "> When repeated several times, you get several bootstrap samples.  \n",
    "> R creates several bootstrap samples using `rep_sample_n(size = n, replace = TRUE, reps = ...)`  \n",
    "5. Calculate the bootstrap point estimate (e.g., mean, median, proportion, slope, etc.) of the $n$ observations in your bootstrap sample  \n",
    "6. Repeat steps (1) - (5) many times to create a distribution of point estimates (the bootstrap distribution)  \n",
    "7. Calculate the plausible range of values around our observed point estimate.  \n",
    "> Step 7 is accomplished using the following code:  \n",
    ">```{r}\n",
    ">rep_sample_n(size = n, replace = TRUE, reps = ...) |>\n",
    "        group_by(replicate) |>\n",
    "        summarize(mean = mean(...))\n",
    ">```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec45ed6-b146-4c61-8e39-c59772f6bf78",
   "metadata": {},
   "source": [
    "#### **<u>Example Code:</u>**\n",
    "\n",
    "\n",
    "```{r}\n",
    "------------------------------------------------------------------------------------\n",
    "BOOTSTRAP (SAMPLE) DISTRIBUTIONS for 6 bootstrap samples\n",
    "------------------------------------------------------------------------------------\n",
    "boot20000 <- one_sample |>\n",
    "  rep_sample_n(size = 40, replace = TRUE, reps = 20000)\n",
    "\n",
    "\n",
    "#Let’s take a look at histograms of the first six replicates of our bootstrap samples.\n",
    "six_bootstrap_samples <- boot20000 |>\n",
    "  filter(replicate <= 6)\n",
    "\n",
    "ggplot(six_bootstrap_samples, aes(price)) +\n",
    "  geom_histogram(fill = \"dodgerblue3\", color = \"lightgrey\") +\n",
    "  xlab(\"Price per night ($)\") +\n",
    "  facet_wrap(~replicate) +\n",
    "  ggtitle(\"Bootstrap Sample Distribution for 6 bootstrap samples\") +\n",
    "  theme(text = element_text(size = 20))\n",
    "\n",
    "six_bootstrap_samples |>\n",
    "  group_by(replicate) |>\n",
    "  summarize(mean = mean(price))\n",
    "\n",
    "# We can see that the bootstrap sample distributions and the sample means are different. \n",
    "# They are different because we are sampling with replacement.\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "BOOTSTRAP (SAMPLING) DISTRIBUTION\n",
    "------------------------------------------------------------------------------------\n",
    "boot20000 <- one_sample |>\n",
    "  rep_sample_n(size = 40, replace = TRUE, reps = 20000)\n",
    "\n",
    "boot20000_means <- boot20000 |>\n",
    "  group_by(replicate) |>\n",
    "  summarize(mean = mean(price))\n",
    "boot20000_means\n",
    "\n",
    "boot_est_dist <- ggplot(boot20000_means, aes(x = mean)) +\n",
    "  geom_histogram(fill = \"dodgerblue3\", color = \"lightgrey\") +\n",
    "  xlab(\"Sample mean price per night ($)\") +\n",
    "  ggtitle(\"Bootstrap Distribution\") +\n",
    "  theme(text = element_text(size = 20))\n",
    "boot_est_dist\n",
    "\n",
    "```\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd796bc3-38b9-4fd1-9e51-21fe88c6fa97",
   "metadata": {},
   "source": [
    "### 12.4 True sampling distribution vs Bootstrap (sampling) distribution:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec89da-d6d7-4866-91fd-0d5a0d8c606c",
   "metadata": {},
   "source": [
    "1) The shape and spread of the true sampling distribution and the bootstrap distribution are similar:  \n",
    "   - The bootstrap distribution lets us get a sense of the point estimate’s variability. \n",
    "   - Therefore, in real life, where we only have one sample and cannot create a sampling distribution, the distribution of the bootstrap sample estimates (here means) can suggest how we might expect our point estimate to behave if we took another sample. \n",
    "\n",
    "\n",
    "2) The mean of the bootstrap sampling distribution is not the same value as the mean of the sampling distribution because the bootstrap sampling distribution was created from samples drawn from a single sample, whereas the sampling distribution was created from samples drawn from the population.  \n",
    "    - The sampling distribution is centred at \\\\$154.51, the **population mean** value  \n",
    "    - The bootstrap distribution is centred at the **original sample mean's** price per night, \\\\$149.56.\n",
    "\n",
    ">Because we are resampling from the original sample repeatedly, we see that the bootstrap distribution is centred at the original sample’s mean value (unlike the sampling distribution of the sample mean, which is centred at the population parameter value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db109b-d62f-47af-aa57-c721c057060b",
   "metadata": {},
   "source": [
    "### 12.5 Important concepts\n",
    "___\n",
    "\n",
    "- **Repeated bootstrap samples** (re-samples with replacement) form the **bootstrap sampling distribution**.\n",
    "- **Bootstrap (sampling) distribution** estimates the **(true) sampling distribution**.\n",
    "- **Standard deviation** of the **bootstrap sampling distribution** estimates the **standard error** of the **(true) sampling distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606a8c8-3cbd-4547-ba8c-563089bf1ff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 12.6 What is a Confidence Interval\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e9c8c-bdee-40c1-b547-edcb4c9001b7",
   "metadata": {},
   "source": [
    "- One should think of a **confidence interval** as a **range of plausible values** for the **population parameter**, which may or may not fall within the interval. This is significantly different than a **point estimate**, which is a **single plausible value** for the **population parameter**.\n",
    "- We can interpret a 90% confidence interval as: we are 90% confident that the true mean is captured by the interval. Or, in other words, across all 90% confidence intervals that could be calculated for the mean of the population of interest, we can expect that 90% of the intervals contain the true mean.\n",
    "\n",
    "> _**Incorrect interpretation:**_ There is a 90% chance that the true parameter of the population will fall within this interval.\n",
    "\n",
    "- A higher confidence level corresponds to a wider range of the interval, and a lower confidence level corresponds to a narrower range.\n",
    "- Choosing the level of confidence depends on the application: i.e If our decision impacts human life and the implications of being wrong are deadly, we may want to be very confident and choose a higher confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a54cc7-ffd2-4579-82f0-2884500386cc",
   "metadata": {},
   "source": [
    "### 12.7 How to calculate a 95% percentile bootstrap confidence interval\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e582d6-f4df-4ee1-ac66-3d9701746487",
   "metadata": {},
   "source": [
    "1) Arrange the observations in the bootstrap distribution in ascending order\n",
    "2) Find the value such that 2.5% of observations fall below it (the 2.5% percentile). Use that value as the lower bound of the interval\n",
    "3) Find the value such that 97.5% of observations fall below it (the 97.5% percentile). Use that value as the upper bound of the interval\n",
    "4) Our interval will capture the middle 95% of the sample mean prices in the bootstrap distribution.\n",
    "\n",
    "##### **<u>Example Code:</u>**\n",
    "\n",
    "```{r}\n",
    "bounds <- boot20000_means |>\n",
    "  select(mean) |>\n",
    "  pull() |>\n",
    "  quantile(c(0.025, 0.975))\n",
    "bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5e9cd-b06f-4972-b62c-a0ac1d8dc3f1",
   "metadata": {},
   "source": [
    "##### **<u>How to report the POINT ESTIMATE and its associated CONFIDENCE INTERVAL:</u>**\n",
    "\n",
    "The sample mean price-per-night of 40 Airbnb listings was \\\\$149.68, and we are 95% “confident” that the true population mean price-per-night for all Airbnb listings in Vancouver is between \\\\$(127.5, 174.93).\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
